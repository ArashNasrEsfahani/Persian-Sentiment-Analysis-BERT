# Persian Sentiment Analysis: Comparing ParsBERT vs. Multilingual BERT ğŸ¤–ğŸ“

**ğŸ“š Course:** Natural Language Processing
**ğŸ›ï¸ Institution:** University of Tehran â€“ Faculty of Mathematics, Statistics, and Computer Science

---

## ğŸ” Project Overview

This project presents a comparative analysis of two transformer-based models for **sentiment analysis in Persian**:

*  **ParsBERT**: A Persian-specific language model.
*  **Multilingual BERT**: A general-purpose multilingual model.

The goal is to **demonstrate the advantage of using a domain-specific model** for tasks in a specific language. I also evaluate how ll each model generalizes to **out-of-domain data**.

ğŸ““ The full implementation is available in `Sentiment_Analysis_in_Persian.ipynb`.

---

## ğŸ§ª Methodology

### 1ï¸âƒ£ Models Used

Two pre-trained models from the ğŸ¤— Hugging Face Hub:

* ğŸŸ£ **ParsBERT**: `HooshvareLab/bert-fa-base-uncased` â€” specifically trained on a massive Persian corpus.
* ğŸ”µ **Multilingual BERT**: `bert-base-uncased` â€” trained on 104 languages.

### 2ï¸âƒ£ Fine-Tuning ğŸ”§

Both models were fine-tuned on the **Snappfood dataset** ğŸ”, which contains:

* **7,000 Persian user reviews**
* Binary labels: Positive (1) / Negative (0)

### 3ï¸âƒ£ Generalization Test ğŸŒ

To test robustness, both models are evaluated on a new, unseen dataset evaluated:

* ğŸ“¦ **Digikala product reviews**
* **6,910 reviews**, different domain
* Tests each modelâ€™s ability to **generalize learned sentiment patterns**

---

## ğŸ“Š Results

### âœ… In-Domain: Snappfood Validation Set

ParsBERT outperformed the multilingual BERT across all metrics.

| ğŸ§  Model     | âœ… Accuracy | ğŸ¯ F1-Score | ğŸ“ˆ AUC   |
| ------------ | ---------- | ----------- | -------- |
| **ParsBERT** | **86.01%** | **86.00%**  | **0.97** |
| BERT         | 82.36%     | 82.28%      | 0.90     |


---

### ğŸŒ Out-of-Domain: Digikala Test Set

The gap widened in the generalization test. ParsBERTâ€™s Persian-specific knowledge proved essential.

| ğŸ§  Model     | âœ… Accuracy | ğŸ¯ F1-Score | ğŸ“ˆ AUC    |
| ------------ | ---------- | ----------- | --------- |
| **ParsBERT** | **93.13%** | **93.11%**  | **0.976** |
| BERT         | 80.59%     | 81.22%      | 0.905     |


---

## ğŸ§  Conclusion

The experiments **strongly validate the effectiveness of domain-specific language models**:

> âœ… **ParsBERT significantly outperforms** the generic multilingual BERT in both in-domain and out-of-domain scenarios.

* ğŸ“Œ Language-specific pre-training matters.
* ğŸ’¡ ParsBERT generalizes better across domains.
* ğŸš€ For Persian NLP, ParsBERT is clearly the better choice for real-world applications.

---

## âš™ï¸ How to Run

### ğŸŒ€ Step 1: Clone the repository

```bash
git clone https://github.com/ArashNasrEsfahani/Persian-Sentiment-Analysis-BERT
cd Persian-Sentiment-Analysis-BERT
```

### ğŸ§± Step 2: Set up the environment

```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```

### ğŸ“’ Step 3: Launch the notebook

```bash
jupyter lab Sentiment_Analysis_in_Persian.ipynb
```

